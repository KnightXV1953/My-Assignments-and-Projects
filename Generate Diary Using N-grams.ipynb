{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"dataset\"  \n",
    "content_list = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):  \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            content = file.read()\n",
    "            content_list.append(content)  \n",
    "content_string = \" \".join(content_list) # convert to string\n",
    "\n",
    "\n",
    "def preprocess_text_first(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r\"^\\d+\\.\\s*\", \"\", text, flags=re.MULTILINE)  # to remove the numbers from start of sentences\n",
    "    text = text.replace('\\n' , '_ ')\n",
    "    return text\n",
    "\n",
    "def preprocess_text_second(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r\"^\\d+\\.\\s*\", \"\", text, flags=re.MULTILINE)  # to remove the numbers from start of sentences\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two pre-processed texts one with '_' and second without '_' \n",
    "processedtext = preprocess_text_first(content_string)\n",
    "processedtext2 = preprocess_text_second(content_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(processedtext) # To seperate starting words and ending words\n",
    "tokens2 = word_tokenize(processedtext2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Starting Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_word = []\n",
    "for i in range(len(tokens)):\n",
    "    if '_' in tokens[i]:\n",
    "        if tokens[i+1].isalpha():\n",
    "            starting_word.append(tokens[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Ending Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_words = []\n",
    "for i in range(len(tokens)):\n",
    "    if '_' in tokens[i]:\n",
    "        ending_words.append(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ending_words = []\n",
    "for word in ending_words:\n",
    "    cleaned_word = word.replace('_', '').replace('.','')\n",
    "    \n",
    "    if cleaned_word:\n",
    "        cleaned_ending_words.append(cleaned_word)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Counts of Unigram  , Bigram , Trigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigram(tokens2):\n",
    "    count = nltk.FreqDist(tokens2)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram(tokens2):\n",
    "    bigrams = nltk.ngrams(tokens2 , 2)\n",
    "    bigrams_count = nltk.FreqDist(bigrams)\n",
    "    return bigrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram(tokens2):\n",
    "    trigrams = nltk.ngrams(tokens2 , 3)\n",
    "    trigrams_count = nltk.FreqDist(trigrams)\n",
    "    return trigrams_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'aur': 472, 'ki': 393, 'ke': 328, 'phir': 324, 'ka': 243, 'baad': 221, 'se': 213, 'k': 202, 'or': 201, 'mein': 181, ...})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams = build_unigram(tokens2)\n",
    "unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Random Sentences using Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentences_unigram(noofsentence = 5):\n",
    "    diary = []\n",
    "    sentence = ''\n",
    "\n",
    "    for i in range(noofsentence):   \n",
    "        sentence = random.choice(starting_word)\n",
    "        \n",
    "        random_number = random.randint(7,12)\n",
    "        \n",
    "        for j in range(random_number-1):\n",
    "            frequent_random_word = random.choices(list(unigrams.keys()), weights=unigrams.values())[0]  # Choose based on frequency\n",
    "            sentence += ' '+(frequent_random_word)\n",
    "        diary.append(sentence)\n",
    "    return \"\\n\".join(diary)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phir keen baare university kaam lunch university cafe gya me dostoun\n",
      "raat kisi se main mujhe unhon or se khaya plans\n",
      "kaam kaam nashtay wapas 11 se lene subah tak coffee\n",
      "us kya parha ma gi torhi 20 kiya ke kae main futsal\n",
      "ajj dip hm tha mausam karnay gayi deir gaye\n"
     ]
    }
   ],
   "source": [
    "print(generate_random_sentences_unigram(5))  # Generates 5 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('ke', 'baad'): 63, ('aur', 'phir'): 55, ('kiya', 'aur'): 49, ('ki', 'namaz'): 49, ('k', 'baad'): 46, ('chala', 'gaya'): 45, ('ki', 'aur'): 44, ('ke', 'liye'): 44, ('or', 'phir'): 42, ('khana', 'khaya'): 39, ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = build_bigram(tokens2)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Random Sentences using Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentences_bigram(noofsentence=5):\n",
    "    diary = []    \n",
    "    for i in range(noofsentence):   \n",
    "        sentence = [random.choice(starting_word)]  # Choose a starting word\n",
    "        \n",
    "        random_number = random.randint(7, 12)  # Sentence length\n",
    "    \n",
    "        for j in range(random_number-1):\n",
    "            probabilities = {}\n",
    "            current_word = sentence[-1]\n",
    "            relevent_bigrams = []\n",
    "            for pair , freq in bigrams.items():\n",
    "                if pair[0] == current_word:\n",
    "                    relevent_bigrams.append((pair[1] , freq))\n",
    "                    \n",
    "            # print(relevent_bigrams)\n",
    "           \n",
    "            total_count = 0\n",
    "            for i , freq in relevent_bigrams:\n",
    "                total_count += freq\n",
    "                \n",
    "            \n",
    "            words = []\n",
    "            probabilities = []\n",
    "            for pair, freq in relevent_bigrams:\n",
    "                words.append(pair)\n",
    "                probabilities.append(freq / total_count)  # Compute probabilities\n",
    "            \n",
    "            next_word = random.choices(words, weights=probabilities)[0]\n",
    "            sentence.append(next_word)\n",
    "        \n",
    "        diary.append(\" \".join(sentence))\n",
    "    \n",
    "    return \"\\n\".join(diary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nashta kia 730 utha or phir a\n",
      "uskai baad hum sary dost mil kr lia nikli\n",
      "phir ghar aakr khana khaya lunch break thi\n",
      "uske baad lagataar classes attend ki class cloud computing ki aur soogaau\n",
      "ghar walon ke ooper thori discussion huwi\n"
     ]
    }
   ],
   "source": [
    "print(generate_random_sentences_bigram(5))  # Generates 5 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('ki', 'namaz', 'parhi'): 16, ('so', 'gaya', '1'): 13, ('namaz', 'ada', 'ki'): 13, ('raat', 'ka', 'khana'): 12, ('university', 'ke', 'liye'): 12, ('khana', 'khaya', 'aur'): 11, ('ki', 'namaz', 'ada'): 11, ('nashta', 'kiya', 'aur'): 10, ('namaz', 'parhi', 'aur'): 10, ('ka', 'khana', 'khaya'): 10, ...})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = build_trigram(tokens2)\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Random Sentences using Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentences_trigram(noofsentence=5):\n",
    "    diary = []    \n",
    "    for i in range(noofsentence):\n",
    "           \n",
    "        # Bigram code for appending second word in sentence list for Trigram model to work\n",
    "        \n",
    "        first_word = random.choice(starting_word)  # Choose a starting word\n",
    "        sentence = [first_word]\n",
    "        relevent_bigrams = []\n",
    "        for pair , freq in bigrams.items():\n",
    "            if pair[0] == first_word:\n",
    "                relevent_bigrams.append((pair[1] , freq))\n",
    "        if relevent_bigrams:\n",
    "            total_count = 0\n",
    "            for i , freq in relevent_bigrams:\n",
    "                total_count += freq\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            for word , freq in relevent_bigrams:\n",
    "                words.append(word)\n",
    "                probabilities.append(freq / total_count)\n",
    "            second_word = random.choices(words , weights=probabilities)[0]\n",
    "        else:\n",
    "            second_word = random.choice(starting_word)\n",
    "            \n",
    "        sentence.append(second_word)\n",
    "        \n",
    "        \n",
    "        random_number = random.randint(7, 12)  # Sentence length\n",
    "        # Trigram model \n",
    "        \n",
    "        for j in range(random_number-1):\n",
    "            probabilities = {}\n",
    "            current_pair = (sentence[-2], sentence[-1]) \n",
    "            relevant_trigrams = []\n",
    "            \n",
    "            \n",
    "            for pair , freq in trigrams.items():\n",
    "                if(pair[0] , pair[1]) == current_pair:\n",
    "                    relevant_trigrams.append((pair[2] , freq))\n",
    "        \n",
    "            total_count = 0\n",
    "            for i , freq in relevant_trigrams:\n",
    "                total_count += freq\n",
    "            \n",
    "            words = []\n",
    "            probabilities = []\n",
    "            for word , freq in relevant_trigrams:\n",
    "                words.append(word)\n",
    "                probabilities.append(freq / total_count)\n",
    "            next_word = random.choices(words, weights=probabilities)[0]\n",
    "            sentence.append(next_word)\n",
    "        \n",
    "        diary.append(\" \".join(sentence))\n",
    "    return \"\\n\".join(diary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaming keh baad dawai li aur sogia so\n",
      "phir bhai ko school chora or phir baaqi classes li\n",
      "ke bd mein ne project ko mukl krna tha\n",
      "ghar per nahi tha toh raat ka khana khanay\n",
      "us k bad neurologist ko refer ker dia unho\n"
     ]
    }
   ],
   "source": [
    "print(generate_random_sentences_trigram(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentence_backward_bigram(noofsentences = 5):\n",
    "    diary = []\n",
    "    for i in range(noofsentences):\n",
    "        sentence = [random.choice(cleaned_ending_words)]\n",
    "        random_number = random.randint(7 , 12)\n",
    "        \n",
    "        for j in range(random_number-1):\n",
    "            current_word = sentence[0]\n",
    "            relevent_bigrams = []\n",
    "            for pair , freq in bigrams.items():\n",
    "                if pair[1] == current_word:\n",
    "                    relevent_bigrams.append((pair[0] , freq))\n",
    "        \n",
    "            if not relevent_bigrams:\n",
    "                break\n",
    "\n",
    "            count = 0\n",
    "            for i , freq in relevent_bigrams: # Get total count\n",
    "                count += freq\n",
    "                \n",
    "            words = []\n",
    "            probabilities = []\n",
    "            \n",
    "            for word , freq in relevent_bigrams:\n",
    "                words.append(word)\n",
    "                probabilities.append(freq/count)\n",
    "\n",
    "            next_word = random.choices(words, weights=probabilities)[0]\n",
    "            sentence.insert(0, next_word)  # Insert at the beginning for backward bigram model\n",
    "    \n",
    "        \n",
    "        diary.append(\" \".join(sentence))\n",
    "    return \"\\n\".join(diary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agaya aur halka dard tha aur shower le\n",
      "ki koshish ki dieting wali class li\n",
      "730 pae main jaldi jaldi utha or salar ko alwida kaha\n",
      "so gayi 5 beje nikal kar diya tha\n",
      "ghar jaake picture khichwayi or phir se le movie laga\n"
     ]
    }
   ],
   "source": [
    "print(generate_random_sentence_backward_bigram(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi Directional Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentence_bidirectional_bigram(noofsentences):\n",
    "    diary = []\n",
    "    max_backward_prob = 0\n",
    "    max_forward_prob = 0\n",
    "    for i in range(noofsentences):\n",
    "        sentence = [random.choice(tokens2)]\n",
    "        random_number = random.randint(7 , 12)\n",
    "        current_word = sentence[-1]        \n",
    "        for j in range(random_number-1):\n",
    "            \n",
    "            relevent_bigrams_forward = []\n",
    "            relevent_bigrams_backward = []\n",
    "            \n",
    "            for pair , freq in bigrams.items():\n",
    "                if pair[0] == current_word:\n",
    "                    relevent_bigrams_forward.append((pair[1] , freq))\n",
    "                if pair[1] == current_word:\n",
    "                    relevent_bigrams_backward.append((pair[0] , freq))\n",
    "                    \n",
    "            if not relevent_bigrams_forward and not relevent_bigrams_backward:\n",
    "                break\n",
    "        \n",
    "            total_forward = 0\n",
    "            for _ , freq in relevent_bigrams_forward:\n",
    "                total_forward += freq\n",
    "            \n",
    "            total_backward = 0\n",
    "            for _ , freq in relevent_bigrams_backward:\n",
    "                total_backward += freq\n",
    "            \n",
    "            forward_words = []\n",
    "            forward_probabilities = []\n",
    "            for word , freq in relevent_bigrams_forward:\n",
    "                forward_words.append(word)\n",
    "                forward_probabilities.append(freq / total_forward)\n",
    "            \n",
    "            backward_words = []\n",
    "            backward_probabilities = []\n",
    "            for word , freq in relevent_bigrams_backward:\n",
    "                backward_words.append(word)\n",
    "                backward_probabilities.append(freq / total_backward)\n",
    "            \n",
    "            max_forward_prob = max(forward_probabilities)\n",
    "            max_backward_prob = max(backward_probabilities)\n",
    "            \n",
    "            if max_backward_prob > max_forward_prob:\n",
    "                next_word = random.choices(backward_words , weights = backward_probabilities)[0]\n",
    "                sentence.insert(0, next_word)\n",
    "                current_word = sentence[0]\n",
    "            else:\n",
    "                next_word = random.choices(forward_words , weights = forward_probabilities)[0]\n",
    "                sentence.append(next_word)\n",
    "                current_word = sentence[-1]\n",
    "                \n",
    "        diary.append(\" \".join(sentence))\n",
    "    return \"\\n\".join(diary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neend mein ne 12 ya nahi aa farmaye 11 baje uth\n",
      "kei karta raha dopahar ka sath azkaar\n",
      "ki ho university ki namaz or me gya raha phir wajah se\n",
      "kiya aur casual scrolling ki us k client ke\n",
      "mil poohnch kr diya raha raha nahi tha tha tha kr\n"
     ]
    }
   ],
   "source": [
    "print(generate_random_sentence_bidirectional_bigram(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
